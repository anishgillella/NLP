{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple RNN, there is the problem of vanishing gradient where the output is dependent on intial inputsand the initial weights do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM is explicitly designed to avoid long term dependency problem. Components of LSTM:\n",
    "1. Memory Cell\n",
    "2. Forget Gate\n",
    "3. Input Layer\n",
    "4. Output Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory Cell is used to basically remember and forget things based on the context of the input. Memeory cell needs to add or remove information based ont he sentences which keep adding.\n",
    "\n",
    "- During the result of pointwise operation, the resultant vector is structured based on the information it should forget and remember."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forget Cell Operation - The combination of previous weights and current inputs (in form of y=mx+c) are passed into a sigmoid operation gives us an output as 1s or 0s and if the output is similar, then it means the context has not changed and stayed similar and if the there are many 0s in the result then that means has context has changed.\n",
    "- The whole operation is known as Forget Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pointwise operators help in adding or removing information. So after the forget cell operation, the information is added using point wise operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nut shell, Certainly! Let's break down the structure of an LSTM (Long Short-Term Memory) recurrent neural network (RNN) into a simple flowchart:\n",
    "\n",
    "1. **Input Sequence**: The input to the LSTM RNN is a sequence of data points, such as words in a sentence or stock prices over time.\n",
    "\n",
    "2. **Input Gate**:\n",
    "   - **Input Information**: Data enters the LSTM cell.\n",
    "   - **Forget Gate**: Determines how much of the previous cell state to forget.\n",
    "\n",
    "3. **Cell State**:\n",
    "   - **Memory Cell**: Stores information over time. It's like the \"memory\" of the LSTM.\n",
    "   - **Operations**: Addition and multiplication operations adjust the cell state based on input and forget gate outputs.\n",
    "\n",
    "4. **Output Gate**:\n",
    "   - **Update Cell State**: Decides how much of the new cell state to use.\n",
    "   - **Output Information**: Produces the output based on the cell state.\n",
    "\n",
    "5. **Output Sequence**: The LSTM RNN outputs a sequence of predictions or features, corresponding to each input in the sequence.\n",
    "\n",
    "\n",
    "In essence, the LSTM RNN processes input sequences step by step, updating its internal cell state and outputting predictions or features at each step. The input gate controls how new input information is integrated, the forget gate decides what information to discard from the cell state, and the output gate regulates the amount of information passed to the output sequence. This flowchart simplifies the complex operations happening inside the LSTM cell, making it easier to understand the fundamental structure of LSTM RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to https://colah.github.io/posts/2015-08-Understanding-LSTMs/ for detailed understanding of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
