{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most important step of NLP is text preprocessing - converting the sentences into efficient vector representations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using other preprocessing techniques in NLP, most of the output vectors are sparse matrices so to overcome this we use Word Embedding. It concists of a attribute such as feature represenatation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of getting an output of one-hot representation, we use featurized representation. \n",
    "- For example If we used a one hot representation, the output is higher dimension and sparse matrix\n",
    "- But by using feature representation, we have lower dimension and dense matrix.\n",
    "In this case, cosime similarity can be easily implemented to find the similarity between 2 vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature representation in Word Embedding is a way to represent words as numerical vectors (called embeddings) in a high-dimensional space. This allows words with similar meanings or contexts to be close together, making it easier for machines to understand and process natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a big library with millions of books. Each book represents a word in a language. Traditionally, computers would represent each word as a unique ID or code, like a catalog number. But this doesn't capture the meaning or relationship between words.\n",
    "\n",
    "\n",
    "Word Embedding says, \"Hey, let's represent each book (word) as a point in a huge map (vector space)!\" This way, similar books (words) are close together, and computers can understand how they relate to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Dog\" and \"Cat\" would be close together in the vector space because they're both animals.\n",
    "- \"Run\" and \"Sprint\" would be close together because they're similar actions.\n",
    "- \"Apple\" and \"Fruit\" would be close together because they're related concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges Overcome:\n",
    "- High dimensionality: Traditional methods struggled with representing words in high-dimensional spaces. Word Embedding solves this by using techniques like PCA or t-SNE to reduce dimensions.\n",
    "- Sparsity: Traditional methods had trouble representing rare or out-of-vocabulary words. Word Embedding uses techniques like subwording to handle this.\n",
    "- Contextual relationships: Traditional methods struggled to capture contextual relationships between words. Word Embedding uses techniques like skip-gram or CBOW to capture these relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Operations: Word Embedding allows for vector operations like addition and subtraction, which can capture nuanced semantic relationships.\n",
    "\n",
    "Example:\n",
    "- King - Man + Woman = Queen (vector subtraction and addition)\n",
    "- Vector(King) - Vector(Man) + Vector(Woman) ≈ Vector(Queen)\n",
    "This example illustrates how vector operations can capture analogies and relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity and Distance: Word Embedding enables calculating similarity and distance between words using cosine similarity or Euclidean distance.\n",
    "\n",
    "Example:\n",
    "- Cosine Similarity(Vector(Dog), Vector(Cat)) > Cosine Similarity(Vector(Dog), Vector(Car))\n",
    "- Distance(Vector(Apple), Vector(Fruit)) < Distance(Vector(Apple), Vector(Computer))\n",
    "These examples demonstrate how Word Embedding captures semantic similarity and distance between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding can solve such analogies by finding the closest vector to the result of the vector operation: Vector(King) - Vector(Man) + Vector(Woman)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COSINE SIMILARITY\n",
    "Cosine similarity is a way to measure how similar two things are. In the context of Word Embedding, it's used to calculate how similar two words are.\n",
    "\n",
    "Simplest Explanation:\n",
    "Imagine you have two arrows (vectors) pointing in different directions. The cosine similarity measures how much they point in the same direction.\n",
    "- If the arrows point in the same direction, the similarity is high (close to 1).\n",
    "- If the arrows point in opposite directions, the similarity is low (close to -1).\n",
    "- If the arrows point at a 90-degree angle, the similarity is zero (they're not similar at all).\n",
    "\n",
    "Mathematically:\n",
    "Cosine similarity = (A • B) / (|A| |B|)\n",
    "Where:\n",
    "A and B are the two vectors (arrows)\n",
    "A • B is the dot product (how much they point in the same direction)\n",
    "|A| and |B| are the magnitudes (lengths) of the vectors\n",
    "\n",
    "In Simple Terms:\n",
    "- Cosine similarity measures how much two vectors (words) point in the same direction. The higher the similarity, the more alike they are!\n",
    "For example, the cosine similarity between the vectors for \"dog\" and \"cat\" might be 0.8, indicating they're quite similar. Meanwhile, the similarity between \"dog\" and \"car\" might be 0.2, indicating they're not very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
