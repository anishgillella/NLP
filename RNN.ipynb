{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN - RECURRENT NEURAL NETOWRKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you're reading a book, and with each new sentence, you remember bits of the previous ones. This memory helps you understand the story as it unfolds. A Recurrent Neural Network (RNN) operates in a similar way: it processes sequences of data while remembering what it has processed before. This \"memory\" helps it understand and predict what comes next in the sequence.\n",
    "\n",
    "What is an RNN?\n",
    "RNN stands for Recurrent Neural Network, a type of artificial neural network designed to recognize patterns in sequences of data such as text, genomes, handwriting, the spoken word, or numerical time series data emanating from sensors, stocks, and many others.\n",
    "\n",
    "##### How Does It Work?\n",
    "Sequential Input: RNNs process data that comes in sequences. For instance, when you feed a sentence into an RNN, it considers each word (or even each character) one at a time.\n",
    "\n",
    "Memory (Hidden State): Unlike a standard neural network, which processes each input independently, an RNN has a form of internal memory or state that gets updated each time it processes a new input. This state captures information about what has been processed so far, essentially summarizing the previous inputs to help the network understand the context and make predictions.\n",
    "\n",
    "Output: Depending on the task, an RNN can output something at every step (e.g., tagging each word in a sentence with a part-of-speech marker) or it can output something at the end (e.g., what's the sentiment of the sentence).\n",
    "\n",
    "Example of RNN Usage:\n",
    "Consider the task of predicting the next word in a text. An RNN sees the words one by one, and for each word, it updates its memory. By the time it reaches the end of a sentence like \"She enjoys playing,\" its memory contains information about all these words. It then uses this memory to predict the next word (\"tennis,\" perhaps).\n",
    "\n",
    "##### Benefits and Limitations:\n",
    "Benefits:\n",
    "\n",
    "Context Aware: RNNs naturally take into account the sequence of inputs, making them suitable for tasks where context is crucial, such as language translation.\n",
    "Flexible in Input/Output Length: RNNs can handle input sequences of varying lengths, and they can also generate sequences of different lengths.\n",
    "Limitations:\n",
    "\n",
    "Difficulty Handling Long Sequences: In practice, RNNs can struggle with long sequences due to the vanishing gradient problem, where it gets harder to learn and remember the context from early in the sequence.\n",
    "Computationally Intensive: They can be slower to train due to their sequential nature.\n",
    "Despite these challenges, RNNs are widely used because of their effectiveness in dealing with sequences. For very long sequences, other architectures like LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Units) networks, which are special types of RNNs designed to handle long dependencies, are often used to overcome some of the limitations of basic RNNs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORWARD PROPAGATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation in a Recurrent Neural Network (RNN) is a process where the network moves from the input layer, through the hidden layers, to the output layer, all the while retaining memory from previous inputs. This is crucial for tasks where context from previous data points (like previous words in a sentence or previous days in stock prices) is important for making a prediction about the next data point.\n",
    "\n",
    "Here's how the forward propagation works in simple terms:\n",
    "\n",
    "1. **Input Layer**: The process starts with the input sequence. Let's say the sequence is a sentence, and each word in the sentence is fed into the RNN one by one. Each word is typically converted into a numerical form, often called a vector, using techniques like one-hot encoding or word embeddings.\n",
    "\n",
    "2. **Hidden Layers**: The RNN has hidden layers where the actual processing happens. Each hidden layer takes two inputs:\n",
    "   - The current word's vector from the input layer.\n",
    "   - The hidden state (memory) from the previous step.\n",
    "\n",
    "   These two inputs are combined to update the hidden state. The updating is usually done through a mathematical operation involving weights (parameters of the model that are learned during training), biases (another set of parameters), and an activation function (like tanh or ReLU) that helps introduce non-linearities into the model.\n",
    "\n",
    "3. **Output Layer**: After updating the hidden state based on the current input and the previous state, the RNN produces an output. The output at each step can be a direct prediction (like the next word in a sequence) or some transformation of the hidden state. This typically involves another set of weights and possibly another activation function. \n",
    "\n",
    "4. **Recurrent Connection**: Unlike in traditional neural networks, after producing the output, the RNN feeds the current hidden state back into itself as the input for the next step. This recurrence is what allows the network to maintain a memory of previous inputs and make informed predictions based on the full sequence up to that point.\n",
    "\n",
    "5. **Sequence Continues**: This process repeats for each element in the input sequence, each time updating the hidden state based on the current input and the previous state, thereby carrying forward information through the sequence.\n",
    "\n",
    "In essence, forward propagation in an RNN loops through the input data, maintains memory across the data points, and sequentially updates internal states before making predictions based on both the new input and what it has 'remembered' so far. This makes RNNs particularly good for tasks where context and sequence order are critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BACK PROPAGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective is reducing the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward propagation, often referred to as backpropagation, is the process through which neural networks learn and adjust their parameters (weights and biases). It involves calculating the gradient (or change) needed in the network’s weights to minimize the error in its outputs. In the context of Recurrent Neural Networks (RNNs), backward propagation not only adjusts the network based on the error at the current output but also takes into account the impact of past inputs due to the recurrent connections.\n",
    "\n",
    "Here’s how backward propagation works in simple terms, especially in the context of RNNs:\n",
    "\n",
    "1. **Error Calculation**: At the output layer of the RNN, the error is calculated for each output. This error is usually the difference between the predicted output and the actual target value. For classification tasks, a common function to measure this error is cross-entropy loss, while for regression tasks, it might be mean squared error.\n",
    "\n",
    "2. **Gradient Calculation**: Backpropagation then calculates the gradient of the loss function with respect to each weight in the network. Gradients indicate the direction and magnitude that weights need to be adjusted to minimize the error. In RNNs, because the same weights are used at each time step, the gradients at each step need to be accumulated over time.\n",
    "\n",
    "3. **Reverse Process**:\n",
    "   - **Output to Hidden Layer**: Starting from the output layer, the gradient of the loss is used to calculate the gradients of the weights connected between the hidden and output layers.\n",
    "   - **Hidden Layer Through Time**: Because of the recurrent nature of RNNs, you not only consider the current time step but also propagate the gradient back through each previous time step. This is called Backpropagation Through Time (BPTT). It means that the gradient at each time step depends not only on the current gradient but also the gradient from the subsequent time step. This propagation continues back to the start of the sequence.\n",
    "\n",
    "4. **Update Weights and Biases**: Using the gradients calculated, the network's parameters are adjusted. This adjustment is usually made in the opposite direction of the gradient (hence the name 'backpropagation') and is scaled by a factor known as the learning rate. This step is crucial as it helps the RNN learn from the errors and improve its predictions in future forward passes.\n",
    "\n",
    "5. **Iteration Until Convergence**: The forward pass and backward pass are repeated iteratively across multiple epochs (an epoch is a complete pass through the entire training dataset) or until the model's performance ceases to improve significantly.\n",
    "\n",
    "**Difference between Forward and Backward Propagation**:\n",
    "\n",
    "- **Purpose**: Forward propagation spreads input data through the network from the input layer to the output layer to make a prediction. Backward propagation, on the other hand, spreads the error from the output back through the network to adjust the weights and minimize the error.\n",
    "- **Direction**: In forward propagation, data and calculations move forward from the input to the output layers. In backward propagation, the movement is in the reverse direction, from the output back towards the input layers.\n",
    "- **Focus**: Forward propagation is concerned with obtaining predictions and outputs based on the current state of the network weights. Backward propagation focuses on learning and improving the model by updating these weights based on the error of predictions.\n",
    "\n",
    "Together, forward and backward propagation enable neural networks like RNNs to learn from data, remember important aspects across inputs, and make accurate predictions based on sequential or time-dependent data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEMS IN SIMPLE RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vanishing Gradient Problem\n",
    "When training neural networks using backward propagation, gradients of the network’s weights are calculated from the output back towards the input. These gradients are crucial as they tell the network how to change its weights to reduce the error. In deep networks or in long sequences processed by RNNs, every layer (or time step in RNNs) contributes to the gradient used in updating weights. If these gradients are very small (<1), they get smaller and smaller as they are propagated backward through each layer or time step due to multiplication. This is because each layer’s gradient is typically a fraction of the previous layer's gradient.\n",
    "\n",
    "Consequences: When gradients become extremely small, they effectively \"vanish.\" This means that weights in the earlier layers of the network (or earlier time steps in an RNN) hardly change, and the learning either becomes very slow or completely stops. This leads to the earlier layers learning very slowly, which makes it hard for the network to learn long-range dependencies in data (e.g., understanding context in a long text).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploding Gradient Problem\n",
    "Conversely, the exploding gradient problem occurs when the gradients are very large. This can happen in networks with many layers or RNNs processing long sequences. In this case, gradients can grow exponentially large as they are propagated backward through the network, again due to multiplication—this time, values greater than 1 multiply together to create even larger values.\n",
    "\n",
    "Consequences: Large gradients cause huge updates to the network’s weights, leading to an unstable network. In practical terms, this might result in model parameters diverging and the model failing to converge or producing nonsensical outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both these cases, \n",
    "- If the activation function is sigmoid, the derivative is 0 to 1, then it might lead to vanishing gradient when updating weights during back propagation.\n",
    "- If the activation function is ReLU, the derivative is a large value , then it might lead to exploding gradient problem when updating weights during back propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to overcome vanishing gradient and exploding gradient, we use LSTM architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
